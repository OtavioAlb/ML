{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contexto do Problema\n",
    "\n",
    "Dado o conjunto de dados de treinamento:\n",
    "\n",
    "$$\n",
    "X = \\Big\\{(x_1, y_1), (x_2, y_2), \\ldots, (x_N, y_N)\\Big\\},\n",
    "$$\n",
    "\n",
    "onde $\\mathbf{x}^{(i)} \\in \\mathbb{R}^{d}$ e $y^{(i)} \\in \\mathbb{R}$, $; i = 1, 2, \\ldots, N$, considere:\n",
    "\n",
    "\n",
    "\n",
    "- $\\mathbf{x} = (x_1, \\ldots, x_d) \\in \\mathbb{R}^d$, que representa os atributos das observações no espaço de entrada.\n",
    "- Adicionamos uma coordenada artificial $x_0 = 1$, tal que $\\tilde{\\mathbf{x}} = (1, x_1, \\ldots, x_d) \\in \\mathbb{R}^{1+d}$.\n",
    "\n",
    "Nosso objetivo é ajustar um modelo de **regressão linear** para prever os valores $y_i$ a partir de $\\mathbf{x}_i$, assumindo que a relação entre as variáveis $\\mathbf{x}$ e $y$ pode ser aproximada linearmente:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\mathbf{w}^T \\tilde{\\mathbf{x}},\n",
    "$$\n",
    "\n",
    "onde $\\mathbf{w} = (w_0, w_1, \\ldots, w_d) \\in \\mathbb{R}^{1+d}$ é o vetor de pesos.\n",
    "\n",
    "A função de custo a ser minimizada é o **Erro Quadrático Médio (MSE)**, dado por:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N \\Big(y_i - \\hat{y}\\Big)^2.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivação da Função de Custo\n",
    "\n",
    "A função de custo da **regressão linear** é definida como o **Erro Quadrático Médio (MSE)**:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=1}^N \\Big(y_i - \\hat{y}\\Big)^2.\n",
    "$$\n",
    "\n",
    "## Forma Vetorial\n",
    "\n",
    "Definimos:\n",
    "\n",
    "- $\\mathbf{X} \\in \\mathbb{R}^{N \\times (d+1)}$, sendo a matriz de atributos com coordenada artificial $1$ adicionada:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} =\n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1d} \\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2d} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{N1} & x_{N2} & \\cdots & x_{Nd}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "- $\\mathbf{y} \\in \\mathbb{R}^N$, sendo o vetor com os valores observados $y_i$:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_N\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "- $\\mathbf{w} \\in \\mathbb{R}^{d+1}$, sendo o vetor de pesos.\n",
    "\n",
    "Expandindo a fórmula:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}) = \\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^T (\\mathbf{y} - \\mathbf{X}\\mathbf{w}).\n",
    "$$\n",
    "\n",
    "## Gradiente da Função de Custo\n",
    "\n",
    "Para minimizar $J(\\mathbf{w})$, calculamos o gradiente em relação a $\\mathbf{w}$:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}}J(\\mathbf{w}) = \\frac{2}{N} (\\mathbf{X}^{T}\\mathbf{y} -\\mathbf{X}^{T}\\mathbf{X}\\mathbf{w})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = -\\frac{2}{N} \\mathbf{X}^T (\\mathbf{y} - \\mathbf{X}\\mathbf{w}).\n",
    "$$\n",
    "\n",
    "## Condição de Otimalidade\n",
    "\n",
    "Para encontrar o mínimo, igualamos o gradiente a zero:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = 0.\n",
    "$$\n",
    "\n",
    "Portanto:\n",
    "\n",
    "$$\n",
    "-\\frac{2}{N} \\mathbf{X}^T (\\mathbf{y} - \\mathbf{X}\\mathbf{w}) = 0.\n",
    "$$\n",
    "\n",
    "Simplificando:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^T (\\mathbf{y} - \\mathbf{X}\\mathbf{w}) = 0.\n",
    "$$\n",
    "\n",
    "Distribuímos o produto interno:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^T\\mathbf{y} - \\mathbf{X}^T\\mathbf{X}\\mathbf{w} = 0.\n",
    "$$\n",
    "\n",
    "Reorganizamos para isolar $\\mathbf{w}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^T\\mathbf{y} = \\mathbf{X}^T\\mathbf{X}\\mathbf{w}.\n",
    "$$\n",
    "\n",
    "Multiplicamos ambos os lados pela inversa de $\\mathbf{X}^T\\mathbf{X}$ (assumindo que $\\mathbf{X}^T\\mathbf{X}$ é invertível):\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}.\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
